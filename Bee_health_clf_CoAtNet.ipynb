{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torch import nn, sqrt\n",
    "import torch\n",
    "from math import sqrt\n",
    "from MBConv import MBConvBlock\n",
    "from SelfAttention import ScaledDotProductAttention\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "data_dir = r'/media/nghia/DATA/DATA/Bee/bee_imgs/'\n",
    "PATH = r'/media/nghia/DATA/DATA/Bee/bee_imgs/bee_data.csv'\n",
    "img_path = r'/media/nghia/DATA/DATA/Bee/bee_imgs/bee_imgs'\n",
    "\n",
    "df = pd.read_csv(PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "df['health'] = df['health'].map({'healthy': 0,\n",
    "                                 'few varrao, hive beetles': 1,\n",
    "                                 'Varroa, Small Hive Beetles': 2,\n",
    "                                 'ant problems': 3,\n",
    "                                 'hive being robbed': 4,\n",
    "                                 'missing queen': 5})\n",
    "\n",
    "transform = {'train': transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "                                          #transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                               [0.229, 0.224, 0.225])]),\n",
    "\n",
    "             'val': transforms.Compose([transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                             [0.229, 0.224, 0.225])]),\n",
    "\n",
    "             'test': transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                              [0.229, 0.224, 0.225])])}\n",
    "\n",
    "# Check for cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class HoneyBeeDataset(Dataset):\n",
    "    # instance attributes\n",
    "    def __init__(self, df, csv_file, root_dir, transform=None):\n",
    "        self.data = df\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = np.asarray(self.data.iloc[:, 6])\n",
    "        self.transform = transform\n",
    "\n",
    "    # length of the dataset passed to this class method\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # get the specific image and labels given the index\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image_label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = HoneyBeeDataset(df=df,\n",
    "                          csv_file=PATH,\n",
    "                          root_dir=img_path)\n",
    "\n",
    "validation_split = 0.2\n",
    "te_split = 0.5\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "val_split = int(np.floor(validation_split * dataset_size))\n",
    "test_split = int(np.floor(te_split * val_split))\n",
    "train_indices = indices[val_split:]\n",
    "rest_indices = indices[:val_split]\n",
    "val_indices, test_indices = rest_indices[test_split:], rest_indices[:test_split]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset_sizes = {'train': len(train_indices), 'val': len(val_indices), 'test': len(test_indices)}\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dataset = HoneyBeeDataset(df=df,\n",
    "                                csv_file=PATH,\n",
    "                                root_dir=img_path,\n",
    "                                transform=transform['train'])\n",
    "\n",
    "val_dataset = HoneyBeeDataset(df=df,\n",
    "                              csv_file=PATH,\n",
    "                              root_dir=img_path,\n",
    "                              transform=transform['val'])\n",
    "\n",
    "test_dataset = HoneyBeeDataset(df=df,\n",
    "                               csv_file=PATH,\n",
    "                               root_dir=img_path,\n",
    "                               transform=transform['test'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataloaders = {'train': torch.utils.data.DataLoader(train_dataset, batch_size=4, sampler=train_sampler),\n",
    "               'val': torch.utils.data.DataLoader(val_dataset, batch_size=4, sampler=valid_sampler),\n",
    "               'test': torch.utils.data.DataLoader(test_dataset, batch_size=1, sampler=test_sampler)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 224, 224])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, lab = train_dataset.__getitem__(2)\n",
    "img.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    #copy the best model weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch, num_epochs-1))\n",
    "        print(\"=\"*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()*inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CoAtNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self,in_ch,image_size,out_chs=[64,96,192,384,768], output_dim=6):\n",
    "        super().__init__()\n",
    "        self.out_chs=out_chs\n",
    "        self.maxpool2d=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.s0=nn.Sequential(\n",
    "            nn.Conv2d(in_ch,in_ch,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_ch,in_ch,kernel_size=3,padding=1)\n",
    "        )\n",
    "        self.mlp0=nn.Sequential(\n",
    "            nn.Conv2d(in_ch,out_chs[0],kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs[0],out_chs[0],kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.s1=MBConvBlock(ksize=3,input_filters=out_chs[0],output_filters=out_chs[0],image_size=image_size//2)\n",
    "        self.mlp1=nn.Sequential(\n",
    "            nn.Conv2d(out_chs[0],out_chs[1],kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs[1],out_chs[1],kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.s2=MBConvBlock(ksize=3,input_filters=out_chs[1],output_filters=out_chs[1],image_size=image_size//4)\n",
    "        self.mlp2=nn.Sequential(\n",
    "            nn.Conv2d(out_chs[1],out_chs[2],kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_chs[2],out_chs[2],kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.s3=ScaledDotProductAttention(out_chs[2],out_chs[2]//8,out_chs[2]//8,8)\n",
    "        self.mlp3=nn.Sequential(\n",
    "            nn.Linear(out_chs[2],out_chs[3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_chs[3],out_chs[3])\n",
    "        )\n",
    "\n",
    "        self.s4=ScaledDotProductAttention(out_chs[3],out_chs[3]//8,out_chs[3]//8,8)\n",
    "        self.mlp4=nn.Sequential(\n",
    "            nn.Linear(out_chs[3],out_chs[4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_chs[4],out_chs[4])\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(14,43008),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(43008, output_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "        B,C,H,W=x.shape\n",
    "        #stage0\n",
    "        y=self.mlp0(self.s0(x))\n",
    "        y=self.maxpool2d(y)\n",
    "        #stage1\n",
    "        y=self.mlp1(self.s1(y))\n",
    "        y=self.maxpool2d(y)\n",
    "        #stage2\n",
    "        y=self.mlp2(self.s2(y))\n",
    "        y=self.maxpool2d(y)\n",
    "        #stage3\n",
    "        y=y.reshape(B,self.out_chs[2],-1).permute(0,2,1) #B,N,C\n",
    "        y=self.mlp3(self.s3(y,y,y))\n",
    "        y=self.maxpool1d(y.permute(0,2,1)).permute(0,2,1)\n",
    "        #stage4\n",
    "        y=self.mlp4(self.s4(y,y,y))\n",
    "        y=self.maxpool1d(y.permute(0,2,1))\n",
    "        N=y.shape[-1]\n",
    "        y=y.reshape(B,self.out_chs[4],int(sqrt(N)),int(sqrt(N)))\n",
    "        #h = y.view(y.shape[0], -1)\n",
    "        out = self.classifier(y)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "CoAtNet(\n  (maxpool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (maxpool1d): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (s0): Sequential(\n    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (mlp0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (s1): MBConvBlock(\n    (_depthwise_conv): Conv2dStaticSamePadding(\n      64, 64, kernel_size=(3, 3), stride=(1, 1), groups=64, bias=False\n      (static_padding): ZeroPad2d((1, 1, 1, 1))\n    )\n    (_bn1): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n    (_se_reduce): Conv2dStaticSamePadding(\n      64, 16, kernel_size=(1, 1), stride=(1, 1)\n      (static_padding): Identity()\n    )\n    (_se_expand): Conv2dStaticSamePadding(\n      16, 64, kernel_size=(1, 1), stride=(1, 1)\n      (static_padding): Identity()\n    )\n    (_project_conv): Conv2dStaticSamePadding(\n      64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n      (static_padding): Identity()\n    )\n    (_bn2): BatchNorm2d(64, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n    (_swish): MemoryEfficientSwish()\n  )\n  (mlp1): Sequential(\n    (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (s2): MBConvBlock(\n    (_depthwise_conv): Conv2dStaticSamePadding(\n      96, 96, kernel_size=(3, 3), stride=(1, 1), groups=96, bias=False\n      (static_padding): ZeroPad2d((1, 1, 1, 1))\n    )\n    (_bn1): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n    (_se_reduce): Conv2dStaticSamePadding(\n      96, 24, kernel_size=(1, 1), stride=(1, 1)\n      (static_padding): Identity()\n    )\n    (_se_expand): Conv2dStaticSamePadding(\n      24, 96, kernel_size=(1, 1), stride=(1, 1)\n      (static_padding): Identity()\n    )\n    (_project_conv): Conv2dStaticSamePadding(\n      96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n      (static_padding): Identity()\n    )\n    (_bn2): BatchNorm2d(96, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n    (_swish): MemoryEfficientSwish()\n  )\n  (mlp2): Sequential(\n    (0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (s3): ScaledDotProductAttention(\n    (fc_q): Linear(in_features=192, out_features=192, bias=True)\n    (fc_k): Linear(in_features=192, out_features=192, bias=True)\n    (fc_v): Linear(in_features=192, out_features=192, bias=True)\n    (fc_o): Linear(in_features=192, out_features=192, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp3): Sequential(\n    (0): Linear(in_features=192, out_features=384, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=384, out_features=384, bias=True)\n  )\n  (s4): ScaledDotProductAttention(\n    (fc_q): Linear(in_features=384, out_features=384, bias=True)\n    (fc_k): Linear(in_features=384, out_features=384, bias=True)\n    (fc_v): Linear(in_features=384, out_features=384, bias=True)\n    (fc_o): Linear(in_features=384, out_features=384, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mlp4): Sequential(\n    (0): Linear(in_features=384, out_features=768, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=14, out_features=43008, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Linear(in_features=43008, out_features=6, bias=True)\n  )\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CoAtNet(3,224)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/34\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghia/CNU-SEM3/AI_proj/Bee_Health/Eagle-Vision/streamlit/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of size: : [4]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m EPOCHS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m35\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# train\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m model_pre \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexp_lr_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[1;32m     27\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     28\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 29\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m phase\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     32\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/CNU-SEM3/AI_proj/Bee_Health/Eagle-Vision/streamlit/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/CNU-SEM3/AI_proj/Bee_Health/Eagle-Vision/streamlit/lib/python3.8/site-packages/torch/nn/modules/loss.py:1163\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/CNU-SEM3/AI_proj/Bee_Health/Eagle-Vision/streamlit/lib/python3.8/site-packages/torch/nn/functional.py:2996\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   2994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2995\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 2996\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: only batches of spatial targets supported (3D tensors) but got targets of size: : [4]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 35\n",
    "# train\n",
    "model_pre = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}