{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, models\n",
    "from collections import namedtuple\n",
    "\n",
    "data_dir = r'/media/nghia/DATA/DATA/Bee/bee_imgs/'\n",
    "PATH = r'/media/nghia/DATA/DATA/Bee/bee_imgs/bee_data.csv'\n",
    "img_path = r'/media/nghia/DATA/DATA/Bee/bee_imgs/bee_imgs'\n",
    "\n",
    "df = pd.read_csv(PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "df['health'] = df['health'].map({'healthy': 0,\n",
    "                                 'few varrao, hive beetles': 1,\n",
    "                                 'Varroa, Small Hive Beetles': 2,\n",
    "                                 'ant problems': 3,\n",
    "                                 'hive being robbed': 4,\n",
    "                                 'missing queen': 5})\n",
    "\n",
    "transform = {'train': transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "                                          #transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                               [0.229, 0.224, 0.225])]),\n",
    "\n",
    "             'val': transforms.Compose([transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                             [0.229, 0.224, 0.225])]),\n",
    "\n",
    "             'test': transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                              [0.229, 0.224, 0.225])])}\n",
    "\n",
    "# Check for cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class HoneyBeeDataset(Dataset):\n",
    "    # instance attributes\n",
    "    def __init__(self, df, csv_file, root_dir, transform=None):\n",
    "        self.data = df\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = np.asarray(self.data.iloc[:, 6])\n",
    "        self.transform = transform\n",
    "\n",
    "    # length of the dataset passed to this class method\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # get the specific image and labels given the index\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image_label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = HoneyBeeDataset(df=df,\n",
    "                          csv_file=PATH,\n",
    "                          root_dir=img_path)\n",
    "\n",
    "validation_split = 0.2\n",
    "te_split = 0.5\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "val_split = int(np.floor(validation_split * dataset_size))\n",
    "test_split = int(np.floor(te_split * val_split))\n",
    "train_indices = indices[val_split:]\n",
    "rest_indices = indices[:val_split]\n",
    "val_indices, test_indices = rest_indices[test_split:], rest_indices[:test_split]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset_sizes = {'train': len(train_indices), 'val': len(val_indices), 'test': len(test_indices)}\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dataset = HoneyBeeDataset(df=df,\n",
    "                                csv_file=PATH,\n",
    "                                root_dir=img_path,\n",
    "                                transform=transform['train'])\n",
    "\n",
    "val_dataset = HoneyBeeDataset(df=df,\n",
    "                              csv_file=PATH,\n",
    "                              root_dir=img_path,\n",
    "                              transform=transform['val'])\n",
    "\n",
    "test_dataset = HoneyBeeDataset(df=df,\n",
    "                               csv_file=PATH,\n",
    "                               root_dir=img_path,\n",
    "                               transform=transform['test'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataloaders = {'train': torch.utils.data.DataLoader(train_dataset, batch_size=4, sampler=train_sampler),\n",
    "               'val': torch.utils.data.DataLoader(val_dataset, batch_size=4, sampler=valid_sampler),\n",
    "               'test': torch.utils.data.DataLoader(test_dataset, batch_size=1, sampler=test_sampler)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    #copy the best model weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch, num_epochs-1))\n",
    "        print(\"=\"*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()*inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, config, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        block, n_blocks, channels = config\n",
    "        self.in_channels = channels[0]\n",
    "\n",
    "        assert len(n_blocks) == len(channels) == 4\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
    "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n",
    "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n",
    "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
    "\n",
    "    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if self.in_channels != block.expansion * channels:\n",
    "            downsample = True\n",
    "        else:\n",
    "            downsample = False\n",
    "\n",
    "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
    "\n",
    "        for i in range(1, n_blocks):\n",
    "            layers.append(block(block.expansion * channels, channels))\n",
    "\n",
    "        self.in_channels = block.expansion * channels\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.fc(h)\n",
    "\n",
    "        return x\n",
    "\n",
    "#OUTPUT_DIM = 6\n",
    "#model = ResNet(OUTPUT_DIM)\n",
    "#model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3,\n",
    "                               stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3,\n",
    "                               stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "\n",
    "        if downsample:\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1,\n",
    "                             stride = stride, bias = False)\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "            downsample = nn.Sequential(conv, bn)\n",
    "        else:\n",
    "            downsample = None\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        i = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            i = self.downsample(i)\n",
    "\n",
    "        x += i\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])\n",
    "resnet18_config = ResNetConfig(block = BasicBlock,\n",
    "                               n_blocks = [2,2,2,2],\n",
    "                               channels = [64, 128, 256, 512])\n",
    "\n",
    "resnet34_config = ResNetConfig(block = BasicBlock,\n",
    "                               n_blocks = [3,4,6,3],\n",
    "                               channels = [64, 128, 256, 512])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 6\n",
    "resnet18 = ResNet(resnet18_config, output_dim=OUTPUT_DIM)\n",
    "resnet34 = ResNet(resnet34_config, output_dim=OUTPUT_DIM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1,\n",
    "                               stride = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3,\n",
    "                               stride = stride, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n",
    "                               stride = 1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "\n",
    "        if downsample:\n",
    "            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1,\n",
    "                             stride = stride, bias = False)\n",
    "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            downsample = nn.Sequential(conv, bn)\n",
    "        else:\n",
    "            downsample = None\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        i = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            i = self.downsample(i)\n",
    "\n",
    "        x += i\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "resnet50_config = ResNetConfig(block = Bottleneck,\n",
    "                               n_blocks = [3, 4, 6, 3],\n",
    "                               channels = [64, 128, 256, 512])\n",
    "\n",
    "resnet101_config = ResNetConfig(block = Bottleneck,\n",
    "                                n_blocks = [3, 4, 23, 3],\n",
    "                                channels = [64, 128, 256, 512])\n",
    "\n",
    "resnet152_config = ResNetConfig(block = Bottleneck,\n",
    "                                n_blocks = [3, 8, 36, 3],\n",
    "                                channels = [64, 128, 256, 512])\n",
    "\n",
    "resnet50 = ResNet(resnet50_config, OUTPUT_DIM)\n",
    "resnet101 = ResNet(resnet101_config, OUTPUT_DIM)\n",
    "resnet152 = ResNet(resnet152_config, OUTPUT_DIM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = resnet18"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/34\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghia/CNU-SEM3/AI_proj/Bee_Health/Eagle-Vision/streamlit/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8366 Acc: 0.6996\n",
      "val Loss: 0.8756 Acc: 0.7505\n",
      "Epoch: 1/34\n",
      "==========\n",
      "train Loss: 0.5542 Acc: 0.7864\n",
      "val Loss: 0.4558 Acc: 0.8279\n",
      "Epoch: 2/34\n",
      "==========\n",
      "train Loss: 0.4472 Acc: 0.8284\n",
      "val Loss: 0.4729 Acc: 0.8627\n",
      "Epoch: 3/34\n",
      "==========\n",
      "train Loss: 0.3681 Acc: 0.8528\n",
      "val Loss: 0.6484 Acc: 0.7505\n",
      "Epoch: 4/34\n",
      "==========\n",
      "train Loss: 0.3347 Acc: 0.8712\n",
      "val Loss: 0.3871 Acc: 0.8627\n",
      "Epoch: 5/34\n",
      "==========\n",
      "train Loss: 0.2727 Acc: 0.8859\n",
      "val Loss: 0.4719 Acc: 0.8472\n",
      "Epoch: 6/34\n",
      "==========\n",
      "train Loss: 0.2747 Acc: 0.8958\n",
      "val Loss: 0.2635 Acc: 0.9014\n",
      "Epoch: 7/34\n",
      "==========\n",
      "train Loss: 0.2373 Acc: 0.9074\n",
      "val Loss: 0.6926 Acc: 0.8646\n",
      "Epoch: 8/34\n",
      "==========\n",
      "train Loss: 0.2172 Acc: 0.9101\n",
      "val Loss: 0.4148 Acc: 0.8859\n",
      "Epoch: 9/34\n",
      "==========\n",
      "train Loss: 0.1315 Acc: 0.9509\n",
      "val Loss: 0.2105 Acc: 0.9265\n",
      "Epoch: 10/34\n",
      "==========\n",
      "train Loss: 0.1134 Acc: 0.9577\n",
      "val Loss: 0.2478 Acc: 0.9226\n",
      "Epoch: 11/34\n",
      "==========\n",
      "train Loss: 0.1074 Acc: 0.9616\n",
      "val Loss: 0.2370 Acc: 0.9207\n",
      "Epoch: 12/34\n",
      "==========\n",
      "train Loss: 0.0966 Acc: 0.9630\n",
      "val Loss: 0.3151 Acc: 0.9188\n",
      "Epoch: 13/34\n",
      "==========\n",
      "train Loss: 0.0982 Acc: 0.9650\n",
      "val Loss: 0.2790 Acc: 0.9207\n",
      "Epoch: 14/34\n",
      "==========\n",
      "train Loss: 0.0913 Acc: 0.9676\n",
      "val Loss: 0.3108 Acc: 0.9110\n",
      "Epoch: 15/34\n",
      "==========\n",
      "train Loss: 0.0880 Acc: 0.9700\n",
      "val Loss: 0.3276 Acc: 0.9110\n",
      "Epoch: 16/34\n",
      "==========\n",
      "train Loss: 0.0794 Acc: 0.9739\n",
      "val Loss: 0.2185 Acc: 0.9284\n",
      "Epoch: 17/34\n",
      "==========\n",
      "train Loss: 0.0828 Acc: 0.9691\n",
      "val Loss: 0.2678 Acc: 0.9091\n",
      "Epoch: 18/34\n",
      "==========\n",
      "train Loss: 0.0735 Acc: 0.9725\n",
      "val Loss: 0.2433 Acc: 0.9265\n",
      "Epoch: 19/34\n",
      "==========\n",
      "train Loss: 0.0710 Acc: 0.9763\n",
      "val Loss: 0.4755 Acc: 0.9033\n",
      "Epoch: 20/34\n",
      "==========\n",
      "train Loss: 0.0671 Acc: 0.9778\n",
      "val Loss: 0.2460 Acc: 0.9207\n",
      "Epoch: 21/34\n",
      "==========\n",
      "train Loss: 0.0668 Acc: 0.9775\n",
      "val Loss: 0.2542 Acc: 0.9130\n",
      "Epoch: 22/34\n",
      "==========\n",
      "train Loss: 0.0670 Acc: 0.9773\n",
      "val Loss: 0.2603 Acc: 0.9246\n",
      "Epoch: 23/34\n",
      "==========\n",
      "train Loss: 0.0606 Acc: 0.9802\n",
      "val Loss: 0.3374 Acc: 0.9110\n",
      "Epoch: 24/34\n",
      "==========\n",
      "train Loss: 0.0705 Acc: 0.9775\n",
      "val Loss: 0.3370 Acc: 0.9149\n",
      "Epoch: 25/34\n",
      "==========\n",
      "train Loss: 0.0704 Acc: 0.9756\n",
      "val Loss: 0.2404 Acc: 0.9265\n",
      "Epoch: 26/34\n",
      "==========\n",
      "train Loss: 0.0677 Acc: 0.9785\n",
      "val Loss: 0.2607 Acc: 0.9149\n",
      "Epoch: 27/34\n",
      "==========\n",
      "train Loss: 0.0637 Acc: 0.9799\n",
      "val Loss: 0.3696 Acc: 0.9130\n",
      "Epoch: 28/34\n",
      "==========\n",
      "train Loss: 0.0668 Acc: 0.9775\n",
      "val Loss: 0.3058 Acc: 0.9130\n",
      "Epoch: 29/34\n",
      "==========\n",
      "train Loss: 0.0674 Acc: 0.9770\n",
      "val Loss: 0.2174 Acc: 0.9304\n",
      "Epoch: 30/34\n",
      "==========\n",
      "train Loss: 0.0673 Acc: 0.9785\n",
      "val Loss: 0.1927 Acc: 0.9304\n",
      "Epoch: 31/34\n",
      "==========\n",
      "train Loss: 0.0603 Acc: 0.9831\n",
      "val Loss: 0.2796 Acc: 0.9130\n",
      "Epoch: 32/34\n",
      "==========\n",
      "train Loss: 0.0592 Acc: 0.9809\n",
      "val Loss: 0.2521 Acc: 0.9304\n",
      "Epoch: 33/34\n",
      "==========\n",
      "train Loss: 0.0630 Acc: 0.9780\n",
      "val Loss: 0.2318 Acc: 0.9246\n",
      "Epoch: 34/34\n",
      "==========\n",
      "train Loss: 0.0649 Acc: 0.9773\n",
      "val Loss: 0.2822 Acc: 0.9168\n",
      "Best val Acc: 0.930368\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 35\n",
    "# train\n",
    "model_pre = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    # no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['test']:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            true_labels.append(labels.item())\n",
    "            outputs = model_pre(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            pred_labels.append(preds.item())\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "    return (true_labels, pred_labels, running_correct, running_total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 485, Total: 517\n",
      "Test Accuracy:  0.9381044487427466\n"
     ]
    }
   ],
   "source": [
    "true_labels, pred_labels, running_correct, running_total= test_model()\n",
    "print('Correct: {}, Total: {}'.format(running_correct, running_total))\n",
    "print('Test Accuracy: ', (running_correct/running_total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       330\n",
      "           1       0.85      0.87      0.86        69\n",
      "           2       0.84      0.78      0.81        46\n",
      "           3       1.00      1.00      1.00        47\n",
      "           4       0.74      0.87      0.80        23\n",
      "           5       0.33      1.00      0.50         2\n",
      "\n",
      "    accuracy                           0.94       517\n",
      "   macro avg       0.79      0.92      0.82       517\n",
      "weighted avg       0.94      0.94      0.94       517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clf report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}