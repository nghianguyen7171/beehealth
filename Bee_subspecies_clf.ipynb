{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "data_dir = r'/media/nghia/DATA/DATA/Bee/bee_imgs/'\n",
    "PATH = r'/media/nghia/DATA/DATA/Bee/bee_imgs/bee_data.csv'\n",
    "img_path = r'/media/nghia/DATA/DATA/Bee/bee_imgs/bee_imgs'\n",
    "df = pd.read_csv(PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "df['health'] = df['health'].map({'healthy': 0,\n",
    "                                 'few varrao, hive beetles': 1,\n",
    "                                 'Varroa, Small Hive Beetles': 2,\n",
    "                                 'ant problems': 3,\n",
    "                                 'hive being robbed': 4,\n",
    "                                 'missing queen': 5})\n",
    "\n",
    "df['subspecies'] = df['subspecies'].map({'-1': 0,\n",
    "                                         'Italian honey bee': 1,\n",
    "                                         'Russian honey bee': 2,\n",
    "                                         'Carniolan honey bee': 3,\n",
    "                                         '1 Mixed local stock 2': 4,\n",
    "                                         'VSH Italian honey bee': 5,\n",
    "                                         'Western honey bee': 6})\n",
    "\n",
    "transform = {'train': transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "                                          #transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                               [0.229, 0.224, 0.225])]),\n",
    "\n",
    "             'val': transforms.Compose([transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                             [0.229, 0.224, 0.225])]),\n",
    "\n",
    "             'test': transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                              [0.229, 0.224, 0.225])])}\n",
    "\n",
    "# Check for cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class HoneyBeeDataset(Dataset):\n",
    "    # instance attributes\n",
    "    def __init__(self, df, csv_file, root_dir, transform=None):\n",
    "        self.data = df\n",
    "        self.root_dir = root_dir\n",
    "        self.labels = np.asarray(self.data.iloc[:, 5])\n",
    "        self.transform = transform\n",
    "\n",
    "    # length of the dataset passed to this class method\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # get the specific image and labels given the index\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image_label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = HoneyBeeDataset(df=df,\n",
    "                          csv_file=PATH,\n",
    "                          root_dir=img_path)\n",
    "\n",
    "validation_split = 0.2\n",
    "te_split = 0.5\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "val_split = int(np.floor(validation_split * dataset_size))\n",
    "test_split = int(np.floor(te_split * val_split))\n",
    "train_indices = indices[val_split:]\n",
    "rest_indices = indices[:val_split]\n",
    "val_indices, test_indices = rest_indices[test_split:], rest_indices[:test_split]\n",
    "\n",
    "dataset_sizes = {'train': len(train_indices), 'val': len(val_indices), 'test': len(test_indices)}\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dataset = HoneyBeeDataset(df=df,\n",
    "                                csv_file=PATH,\n",
    "                                root_dir=img_path,\n",
    "                                transform=transform['train'])\n",
    "\n",
    "val_dataset = HoneyBeeDataset(df=df,\n",
    "                              csv_file=PATH,\n",
    "                              root_dir=img_path,\n",
    "                              transform=transform['val'])\n",
    "\n",
    "test_dataset = HoneyBeeDataset(df=df,\n",
    "                               csv_file=PATH,\n",
    "                               root_dir=img_path,\n",
    "                               transform=transform['test'])\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(train_dataset, batch_size=4, sampler=train_sampler),\n",
    "               'val': torch.utils.data.DataLoader(val_dataset, batch_size=4, sampler=valid_sampler),\n",
    "               'test': torch.utils.data.DataLoader(test_dataset, batch_size=1, sampler=test_sampler)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    #copy the best model weights\n",
    "    model = model.to(device)\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch, num_epochs-1))\n",
    "        print(\"=\"*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for data in dataloaders[phase]:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()*inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def test_model(model_pre):\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    # no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for data in dataloaders['test']:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            true_labels.append(labels.item())\n",
    "            outputs = model_pre(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            pred_labels.append(preds.item())\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "    return (true_labels, pred_labels, running_correct, running_total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Model\n",
    "from Models import AlexNet, LeNet, Load_VGG16, ResNet\n",
    "OUTPUT_DIM = 7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): ReLU(inplace=True)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): ReLU(inplace=True)\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=50176, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=7, bias=True)\n  )\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AlexNet\n",
    "\n",
    "alex_net = AlexNet(OUTPUT_DIM)\n",
    "alex_net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/34\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghia/CNU-SEM3/AI_proj/Bee_Health/Eagle-Vision/streamlit/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1364 Acc: 0.6029\n",
      "val Loss: 0.7470 Acc: 0.6963\n",
      "Epoch: 1/34\n",
      "==========\n",
      "train Loss: 0.7593 Acc: 0.7163\n",
      "val Loss: 0.5921 Acc: 0.7698\n",
      "Epoch: 2/34\n",
      "==========\n",
      "train Loss: 0.5695 Acc: 0.7818\n",
      "val Loss: 0.5549 Acc: 0.8182\n",
      "Epoch: 3/34\n",
      "==========\n",
      "train Loss: 0.4594 Acc: 0.8226\n",
      "val Loss: 0.3258 Acc: 0.8627\n",
      "Epoch: 4/34\n",
      "==========\n",
      "train Loss: 0.3876 Acc: 0.8492\n",
      "val Loss: 0.4595 Acc: 0.8124\n",
      "Epoch: 5/34\n",
      "==========\n",
      "train Loss: 0.3456 Acc: 0.8586\n",
      "val Loss: 0.2918 Acc: 0.8897\n",
      "Epoch: 6/34\n",
      "==========\n",
      "train Loss: 0.3383 Acc: 0.8702\n",
      "val Loss: 0.3698 Acc: 0.8375\n",
      "Epoch: 7/34\n",
      "==========\n",
      "train Loss: 0.2996 Acc: 0.8792\n",
      "val Loss: 0.2274 Acc: 0.9091\n",
      "Epoch: 8/34\n",
      "==========\n",
      "train Loss: 0.2704 Acc: 0.8869\n",
      "val Loss: 0.2880 Acc: 0.8588\n",
      "Epoch: 9/34\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 35\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.SGD(alex_net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "model_pre = train_model(alex_net, criterion, optimizer, exp_lr_scheduler, num_epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}